{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models with CNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'/tmp/try_flags_ee_5uz5b.c:4:19: fatal error: cudnn.h: No such file or directory\\ncompilation terminated.\\n'\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:04:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "from imp import reload\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import utils; reload(utils)\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear model on predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Dense() layer in this way, we can easily convert the 1,000 predictions given by our model into a probability of dog vs cat--simply train a linear model to take the 1,000 predictions as input, and return dog or cat as output, learning from the Kaggle data. This should be easier and more accurate than manually creating a map from imagenet categories to one dog/cat category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some basic config steps. We copy a small amount of our data into a 'sample' directory, with the exact same structure as our 'train' directory--this is *always* a good idea in *all* machine learning, since we should do all of our initial testing using a dataset small enough that we never have to wait for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscats/sample/\"\n",
    "#path = \"data/dogscats/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process as many images at a time as our graphics card allows. This is a case of trial and error to find the max batch size - the largest size that doesn't give an out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "#batch_size=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start with our VGG 16 model, since we'll be using its predictions and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall approach here will be:\n",
    "\n",
    "1. Get the true labels for every image\n",
    "2. Get the 1,000 imagenet category predictions for every image\n",
    "3. Feed these predictions as input to a simple linear model.\n",
    "\n",
    "Let's start by grabbing training and validation batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n",
      "Found 160 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and resizing the images every time we want to use them isn't necessary - instead we should save the processed arrays. By far the fastest way to save and load numpy arrays is using bcolz. This also compresses the arrays, so we save disk space. Here are the functions we'll use to save and load using bcolz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a simple function that joins the arrays from all the batches - let's use this to grab the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Start Skip here\n",
    "val_data = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_data = get_data(path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 3, 224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+ 'train_data.bc', trn_data)\n",
    "save_array(model_path + 'valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our training and validation data later without recalculating them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load here\n",
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 3, 224, 224)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras returns *classes* as a single column, so we convert to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trn_classes[:4]\n",
    "#trn_labels[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pulls information in from imagenet\n",
    "# SKIP THIS\n",
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+ 'train_lastlayer_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_lastlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our training and validation features later without recalculating them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load from here\n",
    "trn_features = load_array(model_path+'train_lastlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our linear model, just like we did earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1000 inputs, \n",
    "#since that's the saved features, and 2 outputs, \n",
    "#for dog and cat\n",
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s - loss: 0.2455 - acc: 0.9312 - val_loss: 0.1234 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1a1758a20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(trn_features, trn_labels, nb_epoch=1, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_4 (Dense)                  (None, 2)             2002        dense_input_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 2,002\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Viewing model prediction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Keras' *fit()* function conveniently shows us the value of the loss function, and the accuracy, after every epoch (\"*epoch*\" refers to one full run through all training examples). The most important metrics for us to look at are for the validation set, since we want to check for over-fitting. \n",
    "\n",
    "- **Tip**: with our first model we should try to overfit before we start worrying about how to handle that - there's no point even thinking about regularization, data augmentation, etc if you're still under-fitting! (We'll be looking at these techniques shortly).\n",
    "\n",
    "As well as looking at the overall metrics, it's also a good idea to look at examples of each of:\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "\n",
    "Let's see what we, if anything, we can from these (in general, these are particularly useful for debugging problems in the model; since this model is so simple, there may not be too much to learn at this stage.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculate predictions on validation set, so we can find correct and incorrect examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We want both the classes...\n",
    "preds = lm.predict_classes(val_features, batch_size=batch_size)\n",
    "# ...and the probabilities of being a cat\n",
    "probs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\n",
    "probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the filenames for the validation set, so we can view images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Number of images to view for each visualization task\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Helper function to plot images by index in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(path + 'valid/' + filenames[i]) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1. A few correct labels at random\n",
    "correct = np.where(preds==val_labels[:,1])[0]\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, probs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#2. A few incorrect labels at random\n",
    "incorrect = np.where(preds!=val_labels[:,1])[0]\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, probs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#3. The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((preds==0) & (preds==val_labels[:,1]))[0]\n",
    "most_correct_cats = np.argsort(probs[correct_cats])[::-1][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], probs[correct_cats][most_correct_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as above, but dogs\n",
    "correct_dogs = np.where((preds==1) & (preds==val_labels[:,1]))[0]\n",
    "most_correct_dogs = np.argsort(probs[correct_dogs])[:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], 1-probs[correct_dogs][most_correct_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#3. The images we were most confident were cats, but are actually dogs\n",
    "incorrect_cats = np.where((preds==0) & (preds!=val_labels[:,1]))[0]\n",
    "most_incorrect_cats = np.argsort(probs[incorrect_cats])[::-1][:n_view]\n",
    "plots_idx(incorrect_cats[most_incorrect_cats], probs[incorrect_cats][most_incorrect_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3. The images we were most confident were dogs, but are actually cats\n",
    "incorrect_dogs = np.where((preds==1) & (preds!=val_labels[:,1]))[0]\n",
    "most_incorrect_dogs = np.argsort(probs[incorrect_dogs])[:n_view]\n",
    "plots_idx(incorrect_dogs[most_incorrect_dogs], 1-probs[incorrect_dogs][most_incorrect_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "most_uncertain = np.argsort(np.abs(probs-0.5))\n",
    "plots_idx(most_uncertain[:n_view], probs[most_uncertain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Perhaps the most common way to analyze the result of a classification model is to use a [confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/). Scikit-learn has a convenient function we can use for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can just print out the confusion matrix, or we can show a graphical view (which is mainly useful for dependents with a larger number of categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember how we defined our linear model? Here it is again for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do you remember the definition of a fully connected layer in the original VGG?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(4096, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might we wondering, what's going on with that *activation* parameter? Adding an 'activation' parameter to a layer in Keras causes an additional function to be called after the layer is calculated. You'll recall that we had no such parameter in our most basic linear model at the start of this lesson - that's because a simple linear model has no *activation function*. But nearly all deep model layers have an activation function - specifically, a *non-linear* activation function, such as tanh, sigmoid (```1/(1+exp(x))```), or relu (```max(0,x)```, called the *rectified linear* function). Why?\n",
    "\n",
    "The reason for this is that if you stack purely linear layers on top of each other, then you just end up with a linear layer! For instance, if your first layer was ```2*x```, and your second was ```-2*x```, then the combination is: ```-2*(2*x) = -4*x```. If that's all we were able to do with deep learning, it wouldn't be very deep! But what if we added a relu activation after our first layer? Then the combination would be: ```-2 * max(0, 2*x)```. As you can see, that does not simplify to just a linear function like the previous example--and indeed we can stack as many of these on top of each other as we wish, to create arbitrarily complex functions.\n",
    "\n",
    "And why would we want to do that? Because it turns out that such a stack of linear functions and non-linear activations can approximate any other function just as close as we want. So we can **use it to model anything**! This extraordinary insight is known as the *universal approximation theorem*. For a visual understanding of how and why this works, I strongly recommend you read Michael Nielsen's [excellent interactive visual tutorial](http://neuralnetworksanddeeplearning.com/chap4.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer generally needs a different activation function to the other layers--because we want to encourage the last layer's output to be of an appropriate form for our particular problem. For instance, if our output is a one hot encoded categorical variable, we want our final layer's activations to add to one (so they can be treated as probabilities) and to have generally a single activation much higher than the rest (since with one hot encoding we have just a single 'one', and all other target outputs are zero). Our classication problems will always have this form, so we will introduce the activation function that has these properties: the *softmax* function. Softmax is defined as (for the i'th output activation): ```exp(x[i]) / sum(exp(x))```.\n",
    "\n",
    "I suggest you try playing with that function in a spreadsheet to get a sense of how it behaves.\n",
    "\n",
    "We will see other activation functions later in this course - but relu (and minor variations) for intermediate layers and softmax for output layers will be by far the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain last layer's linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original VGG16 network's last layer is Dense (i.e. a linear model) it seems a little odd that we are adding an additional linear model on top of it. This is especially true since the last layer had a softmax activation, which is an odd choice for an intermediate layer--and by adding an extra layer on top of it, we have made it an intermediate layer. What if we just removed the original final layer and replaced it with one that we train for the purpose of distinguishing cats and dogs? It turns out that this is a good idea - as we'll see!\n",
    "\n",
    "We start by removing the last layer, and telling Keras that we want to fix the weights in all the other layers (since we aren't looking to learn new parameters for those other layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful!** Now that we've modified the definition of *model*, be careful not to rerun any code in the previous sections, without first recreating the model from scratch! (Yes, I made that mistake myself, which is why I'm warning you about it now...)\n",
    "\n",
    "Now we're ready to add our new final layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and compile our updated model, and set up our batches to use the preprocessed images (note that now we will also *shuffle* the training batches, to add more randomness when using multiple epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a simple function for fitting models, just to save a little typing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.n, nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now we can use it to train the last layer of our model!\n",
    "\n",
    "(It runs quite slowly, since it still has to calculate all the previous layers in order to know what input to pass to the new final layer. We could precalculate the output of the penultimate layer, like we did for the final layer earlier - but since we're only likely to want one or two iterations, it's easier to follow this alternative approach.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_model(model, batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, go back and look at how little code we had to write in this section to finetune the model. Because this is such an important and common operation, keras is set up to make it as easy as possible. We didn't even have to use any external helper functions in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to save weights of all your models, so you can re-use them later. Be sure to note the git log number of your model when keeping a research journal of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-722e1e470fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0;31m# prepare inputs, delegate logic to _test_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                                check_batch_axis=True, batch_size=None):\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   1015\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996/2000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.9998,  1.    ,  1.    ,  1.    ,  1.    ,  1.    ,  1.    ,  1.    ], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(val_data, batch_size=batch_size)\n",
    "probs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n",
    "probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[968  32]\n",
      " [ 27 973]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAEmCAYAAADiNhJgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHU0YpOwQC7dgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58d45ef7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, {'cat':0, 'dog':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned the new final layer, can we, and should we, fine-tune *all* the dense layers? The answer to both questions, it turns out, is: yes! Let's start with the \"can we\" question..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### An introduction to back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key to training multiple layers of a model, rather than just one, lies in a technique called \"back-propagation\" (or *backprop* to its friends). Backprop is one of the many words in deep learning parlance that is creating a new word for something that already exists - in this case, backprop simply refers to calculating gradients using the *chain rule*. (But we will still introduce the deep learning terms during this course, since it's important to know them when reading about or discussing deep learning.)\n",
    "\n",
    "As you (hopefully!) remember from high school, the chain rule is how you calculate the gradient of a \"function of a function\"--something of the form *f(u), where u=g(x)*. For instance, let's say your function is ```pow((2*x), 2)```. Then u is ```2*x```, and f(u) is ```power(u, 2)```. The chain rule tells us that the derivative of this is simply the product of the derivatives of f() and g(). Using *f'(x)* to refer to the derivative, we can say that: ```f'(x) = f'(u) * g'(x) = 2*u * 2 = 2*(2*x) * 2 = 8*x```.\n",
    "\n",
    "Let's check our calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8*x"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sympy let's us do symbolic differentiation (and much more!) in python\n",
    "import sympy as sp\n",
    "# we have to define our variables\n",
    "x = sp.var('x')\n",
    "# then we can request the derivative or any expression of that variable\n",
    "pow(2*x,2).diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key insight is that the stacking of linear functions and non-linear activations we learnt about in the last section is simply defining a function of functions (of functions, of functions...). Each layer is taking the output of the previous layer's function, and using it as input into its function. Therefore, we can calculate the derivative at any layer by simply multiplying the gradients of that layer and all of its following layers together! This use of the chain rule to allow us to rapidly calculate the derivatives of our model at any layer is referred to as *back propagation*.\n",
    "\n",
    "The good news is that you'll never have to worry about the details of this yourself, since libraries like Theano and Tensorflow (and therefore wrappers like Keras) provide *automatic differentiation* (or *AD*). ***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training multiple layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The code below will work on any model that contains dense layers; it's not just for this VGG model.\n",
    "\n",
    "NB: Don't skip the step of fine-tuning just the final layer first, since otherwise you'll have one layer with random weights, which will cause the other layers to quickly move a long way from their optimized imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we haven't changed our architecture, there's no need to re-compile the model - instead, we just set the learning rate. Since we're training more layers, and since we've already optimized the last layer, we should use a lower learning rate than previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 248s - loss: 0.1453 - acc: 0.9681 - val_loss: 0.1281 - val_acc: 0.9725\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 248s - loss: 0.1433 - acc: 0.9692 - val_loss: 0.1323 - val_acc: 0.9725\n"
     ]
    }
   ],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "fit_model(model, batches, val_batches, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is an extraordinarily powerful 5 lines of code. We have fine-tuned all of our dense layers to be optimized for our specific data set. This kind of technique has only become accessible in the last year or two - and we can already do it in just 5 lines of python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's generally little room for improvement in training the convolutional layers, if you're using the model on natural images (as we are). However, there's no harm trying a few of the later conv layers, since it may give a slight improvement, and can't hurt (and we can always load the previous weights if the accuracy decreases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for layer in layers[12:]: layer.trainable=True\n",
    "K.set_value(opt.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "23000/23000 [==============================] - 615s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "23000/23000 [==============================] - 615s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "23000/23000 [==============================] - 615s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "23000/23000 [==============================] - 615s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "23000/23000 [==============================] - 616s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "11036/23000 [=============>................] - ETA: 310s - loss: 8.0444 - acc: 0.5009"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-96b717d3a7fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-31726ee0a827>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, batches, val_batches, nb_epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     model.fit_generator(batches, samples_per_epoch=batches.n, nb_epoch=nb_epoch, \n\u001b[0;32m----> 3\u001b[0;31m                         validation_data=val_batches, nb_val_samples=val_batches.n)\n\u001b[0m",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jd/anaconda3/lib/python3.5/site-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune3.h5')\n",
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can always load the weights later and use the model to do whatever you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(get_batches(path+'valid', gen, False, batch_size*2), val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
